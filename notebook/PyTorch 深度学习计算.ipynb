{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**《深度学习之 PyTorch 实战》**\n",
        "\n",
        "讲师作者：[土豆老师](https://iphysresearch.github.io)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 深度学习计算\n",
        "\n",
        "- 之前的课程中介绍了包括多层感知机在内的简单深度学习模型的原理和实现。\n",
        "- 下面我们将简要概括深度学习计算的各个重要组成部分，如模型构造、参数的访问和初始化等，自定义层，读取、存储和使用GPU。\n",
        "- 通过本章的学习，我们将能够深入了解模型实现和计算的各个细节，并为在之后章节实现更复杂模型打下坚实的基础。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 模型构造\n",
        "\n",
        "让我们回顾一下在「多层感知机的简洁实现」中含单隐藏层的多层感知机的实现方法。\n",
        "\n",
        "我们首先构造 `Sequential` 实例，然后依次添加两个全连接层。其中第一层的输出大小为 256，即隐藏层单元个数是 256；第二层的输出大小为10，即输出层单元个数是10。我们在上一章的其他节中也使用了 `Sequential` 类构造模型。这里我们介绍另外一种基于 `Module` 类的模型构造方法：它让模型构造更加灵活。\n",
        "\n",
        "其实前面我们陆陆续续已经使用了这些方法了，本节系统介绍一下。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 继承 `Module` 类来构造模型\n",
        "\n",
        "`Module` 类是 `nn` 模块里提供的一个模型构造类，是所有神经网络模块的基类，我们可以继承它来定义我们想要的模型。下面继承 `Module` 类构造本节开头提到的多层感知机。这里定义的 `MLP` 类重载了 `Module` 类的 `__init__` 函数和 `forward` 函数。它们分别用于创建模型参数和定义前向计算。前向计算也即正向传播。\n",
        "\n",
        "> 现在我们开始将 `torch.nn` 单独 import 出来。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    # 声明带有模型参数的层，这里声明了两个全连接层\n",
        "    def __init__(self, **kwargs):\n",
        "        # 调用 MLP 父类 Module 的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数\n",
        "        # 参数，如“模型参数的访问、初始化和共享”一节将介绍的模型参数 params\n",
        "        super(MLP, self).__init__(**kwargs)\n",
        "        self.hidden = nn.Linear(784, 256) # 隐藏层\n",
        "        self.act = nn.ReLU()\n",
        "        self.output = nn.Linear(256, 10)  # 输出层\n",
        "\n",
        "    # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出\n",
        "    def forward(self, x):\n",
        "        a = self.act(self.hidden(x))\n",
        "        return self.output(a)"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "以上的 `MLP` 类中无须定义反向传播函数。系统将通过自动求梯度而自动生成反向传播所需的 `backward` 函数。\n",
        "\n",
        "我们可以实例化 `MLP` 类得到模型变量 `net`。下面的代码初始化 `net` 并传入输入数据 `X` 做一次前向计算。其中，`net(X)` 会调用 `MLP` 继承自 `Module` 类的 `__call__` 函数，这个函数将调用 `MLP` 类定义的 `forward` 函数来完成前向计算。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(2, 784)\n",
        "\n",
        "net = MLP()\n",
        "print(net)\n",
        "\n",
        "net(X)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (act): ReLU()\n",
            "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "tensor([[-1.0750e-02,  6.2731e-02,  2.3870e-01,  1.2594e-02, -1.4003e-02,\n          1.7490e-01, -5.6121e-02, -4.4315e-02,  4.6774e-02,  7.7888e-03],\n        [ 2.6103e-02,  5.8683e-02,  2.0914e-01, -4.7617e-05,  1.3082e-03,\n          1.0819e-01, -5.8243e-02, -1.9335e-01, -7.4059e-02, -7.8000e-02]],\n       grad_fn=<AddmmBackward>)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "小贴士：\n",
        ">`__call__` 类方法可以将实例对象变为了可调用对象。\n",
        "\n",
        "注意，这里并没有将 `Module` 类命名为 Layer（层）或者 Model（模型）之类的名字，这是因为该类是一个可供自由组建的部件。它的子类既可以是一个层（如 PyTorch 提供的 `Linear` 类），又可以是一个模型（如这里定义的 `MLP` 类），或者是模型的一个部分。我们下面通过两个例子来展示它的灵活性。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://vip2.loli.io/2021/03/22/wauQHW8O75xoTjn.png)\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Module` 的子类\n",
        "\n",
        "我们刚刚提到，`Module` 类是一个通用的部件。事实上，PyTorch 还实现了继承自 Module 的可以方便构建模型的类: 如 `Sequential`、`ModuleList` 和 `ModuleDict` 等等。\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Sequential` 类\n",
        "\n",
        "当模型的前向计算为简单串联各个层的计算时，`Sequential` 类可以通过更加简单的方式定义模型。这正是 `Sequential` 类的目的：它可以接收一个子模块的有序字典（`OrderedDict`）或者一系列子模块作为参数来逐一添加 `Module` 的实例，而模型的前向计算就是将这些实例按添加的顺序逐一计算。\n",
        "\n",
        "下面我们实现一个与 `Sequential` 类有相同功能的 `MySequential` 类。这或许可以帮助读者更加清晰地理解 `Sequential` 类的工作机制。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class MySequential1(nn.Module):\n",
        "    from collections import OrderedDict\n",
        "    def __init__(self, *args):\n",
        "        super(MySequential1, self).__init__()\n",
        "        if len(args) == 1 and isinstance(args[0], OrderedDict): # 如果传入的是一个 OrderedDict\n",
        "            for key, module in args[0].items():\n",
        "                self.add_module(key, module)  \n",
        "                # add_module 方法会将 module 添加进 self._modules (一个OrderedDict)\n",
        "        else:  # 传入的是一些 Module\n",
        "            for idx, module in enumerate(args):\n",
        "                self.add_module(str(idx), module)\n",
        "    def forward(self, x):\n",
        "        # self._modules 返回一个 OrderedDict，保证会按照成员添加时的顺序遍历成员\n",
        "        for module in self._modules.values():\n",
        "            x = module(x)\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们用 `MySequential1` 类来实现前面描述的 `MLP` 类，并使用随机初始化的模型做一次前向计算。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "net = MySequential1(\n",
        "        nn.Linear(784, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 10), \n",
        "        )\n",
        "print(net)\n",
        "net(X)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MySequential1(\n",
            "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "tensor([[-0.1562, -0.0066, -0.1133, -0.0786,  0.0905,  0.1620, -0.0796,  0.1559,\n          0.2265, -0.1160],\n        [-0.0826,  0.0676,  0.0339, -0.0423,  0.1989,  0.1573,  0.0096,  0.1382,\n          0.1268, -0.0780]], grad_fn=<AddmmBackward>)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "可以观察到这里 `MySequential` 类的使用跟「多层感知机的简洁实现」中 `Sequential` 类的使用没什么区别。\n",
        "\n",
        "我们可以在这里拓展一下！\n",
        "\n",
        "除了在 `Module` 类中自定义 `__init__` 和 `forward` 类方法实现 `Sequential` 类以外，我们也可以通过直接继承 `Sequential` 类，并使用 `add_module` 类方法来实现模块的串联。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class MySequential2(nn.Sequential):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MySequential2, self).__init__(**kwargs)\n",
        "\n",
        "    def add_module(self, block):\n",
        "        # 这里的 block 变量是 Block 子类的实例，并且我们假定其名字是唯一。\n",
        "        # 在 Block 类中有一个成员变量 _children，它的数据类型就是一个\n",
        "        # 有序字典 OrderedDict。当我们的 MySequential2 初始化的时候，\n",
        "        # 程序背后会自动对所有的成员变量进行初始化的。\n",
        "        self._modules[block] = block\n",
        "\n",
        "    def forward(self, x):\n",
        "        # OrderedDict guarantees that members will be traversed in the order\n",
        "        # they were added\n",
        "        for block in self._modules.values():\n",
        "            x = block(x)\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "net = MySequential2()\n",
        "net.add_module(nn.Linear(784,256))\n",
        "net.add_module(nn.ReLU())\n",
        "net.add_module(nn.Linear(256,10))\n",
        "net(X)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "tensor([[ 0.0279, -0.0259,  0.1756,  0.0264, -0.0179, -0.0488,  0.0588, -0.0762,\n          0.2578, -0.2208],\n        [ 0.0311,  0.0045,  0.0950, -0.1463,  0.1052,  0.1958,  0.0185, -0.2009,\n          0.1820, -0.1906]], grad_fn=<AddmmBackward>)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `ModuleList` 类\n",
        "\n",
        "`ModuleList` 接收一个子模块的列表作为输入，然后也可以类似 `List` 那样进行 `append` 和 `extend` 操作:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.ModuleList([nn.Linear(784, 256), \n",
        "                     nn.ReLU()])\n",
        "net.append(nn.Linear(256, 10)) # 类似 List 的 append 操作\n",
        "print(net[-1])  # 类似 List 的索引访问\n",
        "print(net)\n",
        "# net(X) # 会报 NotImplementedError"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=256, out_features=10, bias=True)\n",
            "ModuleList(\n",
            "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "既然 `Sequential` 和 `ModuleList` 都可以进行列表化构造网络，那二者区别是什么呢。\n",
        "\n",
        "`ModuleList` 仅仅是一个储存各种模块的列表，这些模块之间没有联系也没有顺序（所以不用保证相邻层的输入输出维度匹配），而且没有实现 `forward` 功能需要自己实现，所以上面执行 `net(X)` 会报 `NotImplementedError`；而 `Sequential` 内的模块需要按照顺序排列，要保证相邻层的输入输出大小相匹配，内部 `forward` 功能已经实现。\n",
        "\n",
        "`ModuleList` 的出现只是让网络定义前向传播时更加灵活，见下面官网的例子。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModule, self).__init__()\n",
        "        self.linears = nn.ModuleList([nn.Linear(10, 10) for _ in range(10)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ModuleList can act as an iterable, or be indexed using ints\n",
        "        for i, l in enumerate(self.linears):\n",
        "            x = self.linears[i // 2](x) + l(x)\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "另外，`ModuleList` 不同于一般的 Python 的 list，加入到 `ModuleList` 里面的所有模块的参数会被自动添加到整个网络中，下面看一个例子对比一下。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class Module_ModuleList(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Module_ModuleList, self).__init__()\n",
        "        self.linears = nn.ModuleList([nn.Linear(10, 10)])\n",
        "\n",
        "class Module_List(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Module_List, self).__init__()\n",
        "        self.linears = [nn.Linear(10, 10)]\n",
        "\n",
        "net1 = Module_ModuleList()\n",
        "net2 = Module_List()\n",
        "\n",
        "print(\"net1:\")\n",
        "for p in net1.parameters():\n",
        "    print(p.size())\n",
        "\n",
        "print(\"net2:\")\n",
        "for p in net2.parameters():\n",
        "    print(p)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "net1:\n",
            "torch.Size([10, 10])\n",
            "torch.Size([10])\n",
            "net2:\n"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `ModuleDict` 类\n",
        "\n",
        "`ModuleDict` 接收一个子模块的字典作为输入, 然后也可以类似字典那样进行添加访问操作:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.ModuleDict({\n",
        "    'linear': nn.Linear(784, 256),\n",
        "    'act': nn.ReLU(),\n",
        "})\n",
        "net['output'] = nn.Linear(256, 10) # 添加层\n",
        "print(net['linear']) # 访问某层\n",
        "print(net.output)\n",
        "print(net)\n",
        "# net(X) # 会报 NotImplementedError"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=784, out_features=256, bias=True)\n",
            "Linear(in_features=256, out_features=10, bias=True)\n",
            "ModuleDict(\n",
            "  (act): ReLU()\n",
            "  (linear): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "和 `ModuleList` 一样，`ModuleDict` 实例仅仅是存放了一些模块的字典，并没有定义 `forward` 函数需要自己定义。同样，`ModuleDict` 也与 Python 的 `Dict` 有所不同，ModuleDict 里的所有模块的参数会被自动添加到整个网络中。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 构造复杂的模型\n",
        "\n",
        "虽然上面介绍的这些类可以使模型构造更加简单，且可以不需要定义 `forward` 函数，但直接继承 `Module` 类可以极大地拓展模型构造的灵活性。下面我们构造一个稍微复杂点的网络 `FancyMLP` 。在这个网络中，我们通过 `requires_grad` 创建训练中不被迭代的参数，即常数参数。在前向计算中，除了使用创建的常数参数外，我们还使用 `Tensor` 的函数和 Python 的控制流，并多次调用相同的层。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class FancyMLP(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(FancyMLP, self).__init__(**kwargs)\n",
        "\n",
        "        self.rand_weight = torch.rand((20, 20), requires_grad=False) # 不可训练参数（常数参数）\n",
        "        self.linear = nn.Linear(20, 20)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        # 使用创建的常数参数，以及 torch.nn.functional 中的 relu 函数和 mm 函数\n",
        "        x = torch.nn.functional.relu(torch.mm(x, self.rand_weight.data) + 1)\n",
        "\n",
        "        # 复用全连接层。等价于两个全连接层共享参数\n",
        "        x = self.linear(x)\n",
        "        # 控制流，这里我们需要调用 item 函数来返回标量进行比较\n",
        "        while x.norm().item() > 1:\n",
        "            x /= 2\n",
        "        if x.norm().item() < 0.8:\n",
        "            x *= 10\n",
        "        return x.sum()"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "在这个 `FancyMLP` 模型中，我们使用了常数权重 `rand_weight`（注意它不是可训练模型参数）、做了矩阵乘法操作（`torch.mm`）并重复使用了相同的 `Linear` 层。下面我们来测试该模型的前向计算。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(2, 20)\n",
        "net = FancyMLP()\n",
        "print(net)\n",
        "net(X)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FancyMLP(\n",
            "  (linear): Linear(in_features=20, out_features=20, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "tensor(4.5269, grad_fn=<SumBackward0>)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "因为 `FancyMLP` 和 `Sequential` 类都是 `Module` 类的子类，所以我们可以嵌套调用它们。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class NestMLP(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(NestMLP, self).__init__(**kwargs)\n",
        "        self.net = nn.Sequential(nn.Linear(40, 30), \n",
        "                                 nn.ReLU()) \n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "net = nn.Sequential(NestMLP(), \n",
        "                    nn.Linear(30, 20), \n",
        "                    nn.ReLU())\n",
        "net.add_module(\"FancyMLP\",FancyMLP())\n",
        "\n",
        "X = torch.rand(2, 40)\n",
        "print(net)\n",
        "net(X)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): NestMLP(\n",
            "    (net): Sequential(\n",
            "      (0): Linear(in_features=40, out_features=30, bias=True)\n",
            "      (1): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (1): Linear(in_features=30, out_features=20, bias=True)\n",
            "  (2): ReLU()\n",
            "  (FancyMLP): FancyMLP(\n",
            "    (linear): Linear(in_features=20, out_features=20, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "tensor(-1.5452, grad_fn=<SumBackward0>)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 小结\n",
        "\n",
        "- 可以通过继承 `Module` 类来构造模型。\n",
        "- `Sequential`、`ModuleList`、`ModuleDict` 类都继承自 `Module` 类。\n",
        "- 与 `Sequential` 不同，`ModuleList` 和 `ModuleDict` 并没有定义一个完整的网络，它们只是将不同的模块存放在一起，需要自己定义 `forward` 函数。\n",
        "- 虽然 `Sequential` 等类可以使模型构造更加简单，但直接继承 `Module` 类可以极大地拓展模型构造的灵活性。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 模型参数的访问、初始化和共享\n",
        "\n",
        ">(Restart your kernel here)\n",
        "\n",
        "在「线性回归的简洁实现」中，我们通过 `init` 模块来初始化模型的参数。我们也介绍了访问模型参数的简单方法。本节将深入讲解如何访问和初始化模型参数，以及如何在多个层之间共享同一份模型参数。\n",
        "\n",
        "我们先定义一个与上一节中相同的含单隐藏层的多层感知机。我们依然使用默认方式初始化它的参数，并做一次前向计算。与之前不同的是，在这里我们从 `torch.nn` 中导入了 `init` 模块，它包含了多种模型初始化方法。\n",
        "\n",
        "> 现在我们开始将 `torch.nn.init` 单独 import 出来。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "\n",
        "net = nn.Sequential(nn.Linear(4, 3), nn.ReLU(), nn.Linear(3, 1))  # pytorch已进行默认初始化\n",
        "\n",
        "print(net)\n",
        "X = torch.rand(2, 4)\n",
        "Y = net(X).sum()\n",
        "Y"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=4, out_features=3, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "tensor(-0.4634, grad_fn=<SumBackward0>)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 访问模型参数\n",
        "\n",
        "回忆一下上一节中提到的 `Sequential` 类与 `Module` 类的继承关系。对于 `Sequential` 实例中含模型参数的层，我们可以通过 `Module` 类的 `parameters()` 或者 `named_parameters` 方法来访问所有参数（以迭代器的形式返回），后者除了返回参数 `Tensor` 外还会返回其名字。下面，访问多层感知机 `net` 的所有参数："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(net.parameters()))   # parameters()\n",
        "for param in net.parameters():\n",
        "    print(param.size())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'generator'>\n",
            "torch.Size([3, 4])\n",
            "torch.Size([3])\n",
            "torch.Size([1, 3])\n",
            "torch.Size([1])\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(net.named_parameters()))  # named_parameters()\n",
        "for name, param in net.named_parameters():\n",
        "    print(name, param.size())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'generator'>\n",
            "0.weight torch.Size([3, 4])\n",
            "0.bias torch.Size([3])\n",
            "2.weight torch.Size([1, 3])\n",
            "2.bias torch.Size([1])\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "可见返回的名字自动加上了层数的索引作为前缀。\n",
        "\n",
        "我们再来访问 `net` 中单层的参数。对于使用 `Sequential` 类构造的神经网络，我们可以通过方括号 `[]` 来访问网络的任一层。索引 `0` 表示隐藏层为 `Sequential` 实例最先添加的层。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in net[0].named_parameters():\n",
        "    print(name, param.size(), type(param))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight torch.Size([3, 4]) <class 'torch.nn.parameter.Parameter'>\n",
            "bias torch.Size([3]) <class 'torch.nn.parameter.Parameter'>\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in net[:3].named_parameters():\n",
        "    print(name, param.size(), type(param))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.weight torch.Size([3, 4]) <class 'torch.nn.parameter.Parameter'>\n",
            "0.bias torch.Size([3]) <class 'torch.nn.parameter.Parameter'>\n",
            "2.weight torch.Size([1, 3]) <class 'torch.nn.parameter.Parameter'>\n",
            "2.bias torch.Size([1]) <class 'torch.nn.parameter.Parameter'>\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "因为这里是单层的所以没有了层数索引的前缀。另外返回的 `param` 的类型为 `torch.nn.parameter.Parameter`，其实这是 `Tensor` 的子类，和 `Tensor` 不同的是如果一个 `Tensor` 是 `Parameter`，那么它会自动被添加到模型的参数列表里，来看下面这个例子。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MyModel, self).__init__(**kwargs)\n",
        "        self.weight1 = nn.Parameter(torch.rand(2, 2))\n",
        "        self.weight2 = torch.rand(2, 2)\n",
        "    def forward(self, x):\n",
        "        pass\n",
        "\n",
        "net2 = MyModel()\n",
        "for name, param in net2.named_parameters():\n",
        "    print(name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight1\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "可以看到，上面的代码中 `weight1` 在参数列表中但是 `weight2` 却没在参数列表中。\n",
        "\n",
        "因为 `Parameter` 是 `Tensor`，即 `Tensor` 拥有的属性它都有，比如可以根据 `data` 来访问参数数值，用 `grad` 来访问参数梯度。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "weight_0 = list(net[0].parameters())[0]\n",
        "print(weight_0.data)\n",
        "print(weight_0.grad) # 反向传播前梯度为None\n",
        "\n",
        "Y.backward()\n",
        "print(weight_0.grad)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.4664, -0.3196, -0.0848, -0.2056],\n",
            "        [-0.1335, -0.0332,  0.2118,  0.2264],\n",
            "        [ 0.0436,  0.3163, -0.0357,  0.0619]])\n",
            "None\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0147, -0.0973, -0.0663, -0.1005]])\n"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 初始化模型参数\n",
        "\n",
        "我们在「数值稳定性和模型初始化」中提到了 PyTorch 中 `nn.Module` 的模块参数都采取了较为合理的初始化策略（不同类型的 layer 具体采样的哪一种初始化方法的可参考源代码）。但我们经常需要使用其他方法来初始化权重。PyTorch 的 `init` 模块里提供了多种预设的初始化方法。在下面的例子中，我们将权重参数初始化成均值为 0、标准差为 0.01 的正态分布随机数，并依然将偏差参数清零。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in net.named_parameters():\n",
        "    if 'weight' in name:\n",
        "        init.normal_(param, mean=0, std=0.01)\n",
        "        print(name, param.data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.weight tensor([[-0.0110,  0.0027,  0.0070, -0.0077],\n",
            "        [ 0.0093, -0.0052,  0.0074, -0.0023],\n",
            "        [ 0.0040,  0.0012,  0.0077, -0.0097]])\n",
            "2.weight tensor([[-0.0163, -0.0236,  0.0019]])\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "下面使用常数来初始化权重参数。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in net.named_parameters():\n",
        "    if 'bias' in name:\n",
        "        init.constant_(param, val=0)\n",
        "        print(name, param.data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.bias tensor([0., 0., 0.])\n",
            "2.bias tensor([0.])\n"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 自定义初始化方法\n",
        "\n",
        "有时候我们需要的初始化方法并没有在 `init` 模块中提供。这时，可以实现一个初始化方法，从而能够像使用其他初始化方法那样使用它。在这之前我们先来看看 PyTorch 是怎么实现这些初始化方法的，例如 `torch.nn.init.normal_`：\n",
        "\n",
        "```python\n",
        "def normal_(tensor, mean=0, std=1):\n",
        "    with torch.no_grad():\n",
        "        return tensor.normal_(mean, std)\n",
        "```\n",
        "\n",
        "可以看到这就是一个 inplace 改变 `Tensor` 值的函数，而且这个过程是不记录梯度的。\n",
        "\n",
        "类似的我们来实现一个自定义的初始化方法。在下面的例子里，我们令权重有一半概率初始化为 0，有另一半概率初始化为 `[−10,−5]` 和 `[5,10]` 两个区间里均匀分布的随机数。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weight_(tensor): # 自定义初始化\n",
        "    with torch.no_grad():\n",
        "        tensor.uniform_(-10, 10)\n",
        "        tensor *= (tensor.abs() >= 5).float()\n",
        "\n",
        "for name, param in net.named_parameters():\n",
        "    if 'weight' in name:\n",
        "        init_weight_(param)\n",
        "        print(name, param.data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.weight tensor([[-0.0000, -6.4240, -8.0191, -0.0000],\n",
            "        [-0.0000, -8.6734,  0.0000, -6.4193],\n",
            "        [ 9.3927, -0.0000,  9.1377, -0.0000]])\n",
            "2.weight tensor([[0.0000, 5.9064, -0.0000]])\n"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "此外，我们还知道可以通过改变这些参数的 `data` 来改写模型参数值同时不会影响梯度:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in net.named_parameters():\n",
        "    if 'bias' in name:\n",
        "        param.data += 1\n",
        "        print(name, param.data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.bias tensor([1., 1., 1.])\n",
            "2.bias tensor([1.])\n"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 共享模型参数\n",
        "\n",
        "在有些情况下，我们希望在多个层之间共享模型参数。在上面讲「创建模型中不被迭代的参数」时提到了如何共享模型参数: `Module` 类的 `forward` 函数里多次调用同一个层。此外，如果我们传入 `Sequential` 的模块是同一个 `Module` 实例的话参数也是共享的，下面来看一个例子:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "linear = nn.Linear(1, 1, bias=False)\n",
        "net = nn.Sequential(linear, linear)  # 同一个实例串联\n",
        "print(net)\n",
        "\n",
        "for name, param in net.named_parameters():\n",
        "    init.constant_(param, val=3)\n",
        "    print(name, param.data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=1, out_features=1, bias=False)\n",
            "  (1): Linear(in_features=1, out_features=1, bias=False)\n",
            ")\n",
            "0.weight tensor([[3.]])\n"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "在内存中，这两个线性层其实一个对象:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(id(net[0]) == id(net[1]))\n",
        "print(id(net[0].weight) == id(net[1].weight))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "因为模型参数里包含了梯度，所以在反向传播计算时，这些共享的参数的梯度是累加的:\n",
        "\n",
        "$y = (x \\times w) \\times w, w=3 $"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(1, 1)\n",
        "y = net(x).sum()\n",
        "print(y)\n",
        "\n",
        "y.backward()\n",
        "print(net[0].weight.grad) # 单次梯度是 3，两次所以就是 6"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(9., grad_fn=<SumBackward0>)\n",
            "tensor([[12.]])\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 小结\n",
        "\n",
        "- 有多种方法来访问、初始化和共享模型参数。\n",
        "- 可以自定义初始化方法。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- \n",
        "\n",
        "## 自定义层\n",
        "\n",
        ">(Restart your kernel here)\n",
        "\n",
        "深度学习的一个魅力在于神经网络中各式各样的层，例如全连接层和后面章节中将要介绍的卷积层、池化层与循环层。虽然 PyTorch 提供了大量常用的层，但有时候我们依然希望自定义层。本节将介绍如何使用 `Module` 来自定义层，从而可以被重复调用。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 不含模型参数的自定义层\n",
        "\n",
        "我们先介绍如何定义一个不含模型参数的自定义层。事实上，这和上面「模型构造」中介绍的使用 `Module` 类构造模型类似。下面的 `CenteredLayer` 类通过继承 `Module` 类自定义了一个将输入减掉均值后输出的层，并将层的计算定义在了 `forward` 函数里。这个层里不含模型参数。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class CenteredLayer(nn.Module):   # 不含模型参数\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CenteredLayer, self).__init__(**kwargs)\n",
        "    def forward(self, x):\n",
        "        return x - x.mean()"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以实例化这个层，然后做前向计算。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "layer = CenteredLayer()\n",
        "layer(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "tensor([-2., -1.,  0.,  1.,  2.])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们也可以用它来构造更复杂的模型。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "下面打印自定义层各个输出的均值。因为均值是浮点数，所以它的值是一个很接近 0 的数。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y = net(torch.rand(4, 8))\n",
        "y.mean().item()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "-3.725290298461914e-09"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 含模型参数的自定义层\n",
        "\n",
        "我们还可以自定义含模型参数的自定义层。其中的模型参数可以通过训练学出。\n",
        "\n",
        "在上面「模型参数的访问、初始化和共享」中介绍了 `Parameter` 类其实是 `Tensor` 的子类，如果一个 `Tensor` 是 `Parameter`，那么它会自动被添加到模型的参数列表里。所以在自定义含模型参数的层时，我们应该将参数定义成 `Parameter`，除了像「模型参数的访问、初始化和共享」节那样直接定义成 `Parameter` 类外，还可以使用 `ParameterList` 和 `ParameterDict` 分别定义参数的列表和字典。\n",
        "\n",
        "- `ParameterList` 接收一个 `Parameter` 实例的列表作为输入然后得到一个参数列表，使用的时候可以用索引来访问某个参数，另外也可以使用 `append` 和 `extend` 在列表后面新增参数。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class MyListDense(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyListDense, self).__init__()\n",
        "        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])\n",
        "        self.params.append(nn.Parameter(torch.randn(4, 1)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.params)):\n",
        "            x = torch.mm(x, self.params[i])\n",
        "        return x\n",
        "net = MyListDense()\n",
        "print(net)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyListDense(\n",
            "  (params): ParameterList(\n",
            "      (0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
            "      (1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
            "      (2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
            "      (3): Parameter containing: [torch.FloatTensor of size 4x1]\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `ParameterDict` 接收一个 `Parameter` 实例的字典作为输入然后得到一个参数字典，然后可以按照字典的规则使用了。例如使用 `update()` 新增参数，使用 `keys()` 返回所有键值，使用 `items()` 返回所有键值对等等，可参考[官方文档](https://pytorch.org/docs/stable/nn.html#parameterdict)。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDictDense(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyDictDense, self).__init__()\n",
        "        self.params = nn.ParameterDict({\n",
        "                'linear1': nn.Parameter(torch.randn(4, 4)),\n",
        "                'linear2': nn.Parameter(torch.randn(4, 1))\n",
        "        })\n",
        "        self.params.update({'linear3': nn.Parameter(torch.randn(4, 2))}) # 新增\n",
        "\n",
        "    def forward(self, x, choice='linear1'):\n",
        "        return torch.mm(x, self.params[choice])\n",
        "\n",
        "net = MyDictDense()\n",
        "print(net)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyDictDense(\n",
            "  (params): ParameterDict(\n",
            "      (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
            "      (linear2): Parameter containing: [torch.FloatTensor of size 4x1]\n",
            "      (linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "这样就可以根据传入的键值来进行不同的前向传播："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(1, 4)\n",
        "print(net(x, 'linear1'))\n",
        "print(net(x, 'linear2'))\n",
        "print(net(x, 'linear3'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.4429, -2.9363, -2.8187, -0.3683]], grad_fn=<MmBackward>)\n",
            "tensor([[-1.2958]], grad_fn=<MmBackward>)\n",
            "tensor([[-0.3718, -0.7315]], grad_fn=<MmBackward>)\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们也可以使用自定义层构造模型。它和 PyTorch 的其他层在使用上很类似。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Sequential(\n",
        "    MyDictDense(),\n",
        "    MyListDense(),\n",
        ")\n",
        "print(net)\n",
        "print(net(x))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): MyDictDense(\n",
            "    (params): ParameterDict(\n",
            "        (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
            "        (linear2): Parameter containing: [torch.FloatTensor of size 4x1]\n",
            "        (linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n",
            "    )\n",
            "  )\n",
            "  (1): MyListDense(\n",
            "    (params): ParameterList(\n",
            "        (0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
            "        (1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
            "        (2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
            "        (3): Parameter containing: [torch.FloatTensor of size 4x1]\n",
            "    )\n",
            "  )\n",
            ")\n",
            "tensor([[-23.8090]], grad_fn=<MmBackward>)\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 小结\n",
        "\n",
        "- 可以通过 `Module` 类自定义神经网络中的层，从而可以被重复调用。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 读取和存储\n",
        "\n",
        ">(Restart your kernel here)\n",
        "\n",
        "到目前为止，我们介绍了如何处理数据以及如何构建、训练和测试深度学习模型。然而在实际中，我们有时需要把训练好的模型部署到很多不同的设备。在这种情况下，我们可以把内存中训练好的模型参数存储在硬盘上供后续读取使用。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 读写Tensor\n",
        "\n",
        "我们可以直接使用 `save` 函数和 `load` 函数分别存储和读取 `Tensor`。`save` 使用 Python 的 pickle 实用程序将对象进行序列化，然后将序列化的对象保存到 disk，使用 `save` 可以保存各种对象，包括模型、张量和字典等。而 `load` 使用 unpickle 工具将 pickle 的对象文件反序列化为内存。\n",
        "\n",
        "- 下面的例子创建了 `Tensor` 变量 `x`，并将其存在文件名同为 `x.pt` 的文件里。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "x = torch.ones(3)\n",
        "torch.save(x, 'x.pt')"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh x.pt # 本地目录"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r--  1 herb  staff   351B  3 27 10:12 x.pt\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "然后我们将数据从存储的文件读回内存。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = torch.load('x.pt')\n",
        "x2"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "tensor([1., 1., 1.])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 我们还可以存储一个 `Tensor` 列表并读回内存："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.zeros(4)\n",
        "torch.save([x, y], 'xy.pt')\n",
        "xy_list = torch.load('xy.pt')\n",
        "xy_list"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "[tensor([1., 1., 1.]), tensor([0., 0., 0., 0.])]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 存储并读取一个从字符串映射到 `Tensor` 的字典："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({'x': x, 'y': y}, 'xy_dict.pt')\n",
        "xy = torch.load('xy_dict.pt')\n",
        "xy"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "{'x': tensor([1., 1., 1.]), 'y': tensor([0., 0., 0., 0.])}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 读写模型"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### state_dict\n",
        "\n",
        "在 PyTorch 中，`Module` 的可学习参数(即权重和偏差)，模块模型包含在参数中(通过 `model.parameters()` 访问)。`state_dict` 是一个从参数名称隐射到参数 `Tesnor` 的字典对象。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden = nn.Linear(3, 2)\n",
        "        self.act = nn.ReLU()\n",
        "        self.output = nn.Linear(2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a = self.act(self.hidden(x))\n",
        "        return self.output(a)\n",
        "\n",
        "net = MLP()\n",
        "net.state_dict()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "OrderedDict([('hidden.weight', tensor([[ 0.4776, -0.0531,  0.5554],\n                      [ 0.0822, -0.4127, -0.5208]])),\n             ('hidden.bias', tensor([-0.3275,  0.1323])),\n             ('output.weight', tensor([[ 0.4475, -0.0775]])),\n             ('output.bias', tensor([-0.3367]))])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        ">注意，只有具有可学习参数的层(卷积层、线性层等)才有 `state_dict` 中的条目。优化器(optim)也有一个 `state_dict`，其中包含关于优化器状态以及所使用的超参数的信息。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer.state_dict()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "{'state': {},\n 'param_groups': [{'lr': 0.001,\n   'momentum': 0.9,\n   'dampening': 0,\n   'weight_decay': 0,\n   'nesterov': False,\n   'params': [140575174809928,\n    140575174810000,\n    140575174810072,\n    140575174810144]}]}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 保存和加载模型\n",
        "\n",
        "PyTorch中保存和加载训练模型有两种常见的方法:\n",
        "\n",
        "- 仅保存和加载模型参数 (`state_dict`)；\n",
        "- 保存和加载整个模型。\n",
        "\n",
        "1. 保存和加载模型参数 `state_dict`（**推荐方式**）\n",
        ">由于保存整个模型将耗费大量的存储，故官方推荐只保存参数，然后在建好模型的基础上加载。\n",
        "\n",
        "    保存：\n",
        "    \n",
        "```python\n",
        ">>torch.save(model.state_dict(), PATH) # 推荐的文件后缀名是pt或pth\n",
        "```\n",
        "\n",
        "    加载：\n",
        "    \n",
        "```python\n",
        ">>model = TheModelClass(*args, **kwargs);\n",
        ">>model.load_state_dict(torch.load(PATH));\n",
        "```\n",
        "    \n",
        "2. 保存和加载整个模型\n",
        "\n",
        "    保存：\n",
        "\n",
        "```python\n",
        ">>torch.save(model, PATH)\n",
        "```\n",
        "\n",
        "    加载：\n",
        "    \n",
        "```python\n",
        ">>model = torch.load(PATH)\n",
        "```\n",
        "\n",
        "我们采用推荐的方法一来实验一下:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.randn(2, 3)\n",
        "Y = net(X)\n",
        "\n",
        "PATH = \"./net.pt\"\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "net2 = MLP()\n",
        "net2.load_state_dict(torch.load(PATH))\n",
        "Y2 = net2(X)\n",
        "Y2 == Y"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "tensor([[True],\n        [True]])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "因为这 `net` 和 `net2` 都有同样的模型参数，那么对同一个输入 `X` 的计算结果将会是一样的。上面的输出也验证了这一点。\n",
        "\n",
        "A common PyTorch convention is to save models using either a `.pt` or `.pth` file extension.\n",
        "\n",
        "此外，还有一些其他使用场景，例如 GPU 与 CPU 之间的模型保存与读取、使用多块 GPU 的模型的存储等等，使用的时候可以参考[官方文档](https://pytorch.org/tutorials/beginner/saving_loading_models.html)。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 小结\n",
        "\n",
        "- 通过 `save` 函数和 `load` 函数可以很方便地读写 `Tensor`。\n",
        "- 通过 `save` 函数和 `load_state_dict` 函数可以很方便地读写模型的参数。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## GPU计算\n",
        "\n",
        ">(Restart your kernel here)\n",
        "\n",
        "到目前为止，我们一直在使用 CPU 计算。对复杂的神经网络和大规模的数据来说，使用 CPU 来计算可能不够高效。在本节中，我们将介绍如何使用单块 NVIDIA GPU 来计算。所以需要确保已经安装好了 PyTorch GPU 版本。准备工作都完成后，下面就可以通过 `nvidia-smi` 命令来查看显卡信息了。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi  # 对Linux/macOS用户有效"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Mar 22 09:34:51 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 396.54                 Driver Version: 396.54                    |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  GeForce GTX 108...  Off  | 00000000:03:00.0  On |                  N/A |\n",
            "| 21%   35C    P8    18W / 250W |    951MiB / 11177MiB |     37%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   1  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
            "| 23%   39C    P8    18W / 250W |     12MiB / 11178MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   2  GeForce GTX 108...  Off  | 00000000:81:00.0 Off |                  N/A |\n",
            "| 21%   35C    P8    16W / 250W |     12MiB / 11178MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   3  GeForce GTX 108...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
            "| 21%   34C    P8    16W / 250W |     12MiB / 11178MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "```text\n",
        "Mon Mar 22 09:05:48 2021       \n",
        "+-----------------------------------------------------------------------------+\n",
        "| NVIDIA-SMI 396.54                 Driver Version: 396.54                    |\n",
        "|-------------------------------+----------------------+----------------------+\n",
        "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
        "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
        "|===============================+======================+======================|\n",
        "|   0  GeForce GTX 108...  Off  | 00000000:03:00.0  On |                  N/A |\n",
        "| 21%   35C    P5    23W / 250W |    951MiB / 11177MiB |      8%      Default |\n",
        "+-------------------------------+----------------------+----------------------+\n",
        "|   1  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
        "| 21%   37C    P8    18W / 250W |     12MiB / 11178MiB |      0%      Default |\n",
        "+-------------------------------+----------------------+----------------------+\n",
        "|   2  GeForce GTX 108...  Off  | 00000000:81:00.0 Off |                  N/A |\n",
        "| 21%   34C    P8    16W / 250W |     12MiB / 11178MiB |      0%      Default |\n",
        "+-------------------------------+----------------------+----------------------+\n",
        "|   3  GeForce GTX 108...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
        "| 21%   33C    P8    16W / 250W |     12MiB / 11178MiB |      0%      Default |\n",
        "+-------------------------------+----------------------+----------------------+\n",
        "                                                                               \n",
        "+-----------------------------------------------------------------------------+\n",
        "| Processes:                                                       GPU Memory |\n",
        "|  GPU       PID   Type   Process name                             Usage      |\n",
        "|=============================================================================|\n",
        "+-----------------------------------------------------------------------------+\n",
        "\n",
        "```\n",
        "\n",
        "可以看到我这里只有四块 GTX 1080Ti，每块显存一共有 12000 M。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 计算设备\n",
        "\n",
        "PyTorch 可以指定用来存储和计算的设备，如使用内存的 CPU 或者使用显存的 GPU。默认情况下，PyTorch 会将数据创建在内存，然后利用 CPU 来计算。\n",
        "\n",
        "用 `torch.cuda.is_available()` 查看 GPU 是否可用:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "torch.cuda.is_available() # 输出 True 为 GPU 可用"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "查看 GPU 数量："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.device_count() # 输出 4"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "4"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "查看当前 GPU 索引号，索引号从 0 开始："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.current_device() # 输出 0"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "根据索引号查看 GPU 名字:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_name(0) # 输出 'GeForce GTX 1080 Ti'"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "'GeForce GTX 1080 Ti'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Tensor` 的 GPU 计算\n",
        "\n",
        "默认情况下，`Tensor` 会被存在内存上。因此，之前我们每次打印 `Tensor` 的时候看不到 GPU 相关标识。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1, 2, 3])\n",
        "x"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "tensor([1, 2, 3])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用 `.cuda()` 可以将 CPU 上的 `Tensor` 转换（复制）到 GPU 上。如果有多块 GPU，我们用 `.cuda(i)` 来表示第 i 块 GPU 及相应的显存（i从0开始）且 `cuda(0)` 和 `cuda()` 等价。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.cuda(0)\n",
        "x"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "tensor([1, 2, 3], device='cuda:0')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以通过 `Tensor` 的 `device` 属性来查看该 `Tensor` 所在的设备。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x.device"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "device(type='cuda', index=0)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以直接在创建的时候就指定设备，使用 `torch.device`。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "x = torch.tensor([1, 2, 3], device=device)\n",
        "# or\n",
        "x = torch.tensor([1, 2, 3]).to(device)\n",
        "x"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "tensor([1, 2, 3], device='cuda:0')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果对在 GPU 上的数据进行运算，那么结果还是存放在 GPU 上。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y = x**2\n",
        "y"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "tensor([1, 4, 9], device='cuda:0')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "需要注意的是，存储在不同位置中的数据是不可以直接进行计算的。即存放在 CPU 上的数据不可以直接与存放在 GPU 上的数据进行运算，位于不同 GPU 上的数据也是不能直接进行计算的。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y + x.cpu() # 会报错"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-db4a02fb9c87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 会报错\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.to(\"cuda:1\")\n",
        "y + x  # 也会报错"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-4f4bd6199f25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m  \u001b[0;31m# 也会报错\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0!"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![](https://vip2.loli.io/2021/03/22/tTvcxoVbz5D876w.png)\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "z = x.to(\"cuda:1\")\n",
        "z + y"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "tensor([ 2,  6, 12], device='cuda:1')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 模型的 GPU 计算\n",
        "\n",
        "同 `Tensor` 类似，PyTorch 模型也可以通过 `.cuda` 转换到GPU上。我们可以通过检查模型的参数的 `device` 属性来查看存放模型的设备。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Linear(3, 1)\n",
        "list(net.parameters())[0].device "
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "device(type='cpu')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "可见模型在 CPU 上，将其转换到 GPU 上:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "net.cuda() # Moves all model parameters and buffers to the GPU.\n",
        "list(net.parameters())[0].device"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "device(type='cuda', index=0)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "同样的，我么需要保证模型输入的 `Tensor` 和模型都在同一设备上，否则会报错。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(2,3).cuda()\n",
        "net(x)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "tensor([[-0.6742],\n        [-0.1176]], device='cuda:0', grad_fn=<AddmmBackward>)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 小结\n",
        "\n",
        "- PyTorch 可以指定用来存储和计算的设备，如使用内存的 CPU 或者使用显存的 GPU。在默认情况下，PyTorch 会将数据创建在内存，然后利用 CPU 来计算。\n",
        "- PyTorch 要求计算的所有输入数据都在内存或同一块显卡的显存上。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc-autonumbering": true,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "toc-showtags": false,
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}