{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**《深度学习之 PyTorch 实战》**\n",
    "\n",
    "讲师作者：[土豆老师](https://iphysresearch.github.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 现代卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度卷积神经网络（AlexNet）\n",
    "\n",
    ">(Restart your kernel here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 LeNet 提出后的将近 20 年里，神经网络一度被其他机器学习方法超越，如支持向量机。虽然 LeNet 可以在早期的小数据集上取得好的成绩，但是在更大的真实数据集上的表现并不尽如人意。一方面，神经网络计算复杂。虽然 20 世纪 90 年代也有过一些针对神经网络的加速硬件，但并没有像之后 GPU 那样大量普及。因此，训练一个多通道、多层和有大量参数的卷积神经网络在当年很难完成。另一方面，当年研究者还没有大量深入研究参数初始化和非凸优化算法等诸多领域，导致复杂的神经网络的训练通常较困难。\n",
    "\n",
    "我们在上一节看到，神经网络可以直接基于图像的原始像素进行分类。这种称为端到端（end-to-end）的方法节省了很多中间步骤。然而，在很长一段时间里更流行的是研究者通过勤劳与智慧所设计并生成的手工特征。这类图像分类研究的主要流程是：\n",
    "\n",
    "1. 获取图像数据集；\n",
    "2. 使用已有的特征提取函数生成图像的特征；\n",
    "3. 使用机器学习模型对图像的特征分类。\n",
    "\n",
    "当时认为的机器学习部分仅限最后这一步。如果那时候跟机器学习研究者交谈，他们会认为机器学习既重要又优美。优雅的定理证明了许多分类器的性质。机器学习领域生机勃勃、严谨而且极其有用。然而，如果跟计算机视觉研究者交谈，则是另外一幅景象。他们会告诉你图像识别里“不可告人”的现实是：计算机视觉流程中真正重要的是数据和特征。也就是说，使用较干净的数据集和较有效的特征甚至比机器学习模型的选择对图像分类结果的影响更大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习特征表示\n",
    "\n",
    "既然特征如此重要，它该如何表示呢？\n",
    "\n",
    "我们已经提到，在相当长的时间里，特征都是基于各式各样手工设计的函数从数据中提取的。事实上，不少研究者通过提出新的特征提取函数不断改进图像分类结果。这一度为计算机视觉的发展做出了重要贡献。\n",
    "\n",
    "然而，另一些研究者则持异议。他们认为特征本身也应该由学习得来。他们还相信，为了表征足够复杂的输入，特征本身应该分级表示。持这一想法的研究者相信，多层神经网络可能可以学得数据的多级表征，并逐级表示越来越抽象的概念或模式。以图像分类为例，并回忆「二维卷积层」一节中物体边缘检测的例子。在多层神经网络中，图像的第一级的表示可以是在特定的位置和⻆度是否出现边缘；而第二级的表示说不定能够将这些边缘组合出有趣的模式，如花纹；在第三级的表示中，也许上一级的花纹能进一步汇合成对应物体特定部位的模式。这样逐级表示下去，最终，模型能够较容易根据最后一级的表示完成分类任务。需要强调的是，输入的逐级表示由多层模型中的参数决定，而这些参数都是学出来的。\n",
    "\n",
    "尽管一直有一群执着的研究者不断钻研，试图学习视觉数据的逐级表征，然而很长一段时间里这些野心都未能实现。这其中有诸多因素值得我们一一分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **缺失要素一：数据**\n",
    "\n",
    "    包含许多特征的深度模型需要大量的有标签的数据才能表现得比其他经典方法更好。限于早期计算机有限的存储和 90 年代有限的研究预算，大部分研究只基于小的公开数据集。例如，不少研究论文基于加州大学欧文分校（UCI）提供的若干个公开数据集，其中许多数据集只有几百至几千张图像。这一状况在 2010 年前后兴起的大数据浪潮中得到改善。特别是，2009 年诞生的 ImageNet 数据集包含了 1,000 大类物体，每类有多达数千张不同的图像。这一规模是当时其他公开数据集无法与之相提并论的。ImageNet 数据集同时推动计算机视觉和机器学习研究进入新的阶段，使此前的传统方法不再有优势。\n",
    "\n",
    "- **缺失要素二：硬件**\n",
    "\n",
    "    深度学习对计算资源要求很高。早期的硬件计算能力有限，这使训练较复杂的神经网络变得很困难。然而，通用GPU的到来改变了这一格局。很久以来，GPU 都是为图像处理和计算机游戏设计的，尤其是针对大吞吐量的矩阵和向量乘法从而服务于基本的图形变换。值得庆幸的是，这其中的数学表达与深度网络中的卷积层的表达类似。通用 GPU 这个概念在 2001 年开始兴起，涌现出诸如 OpenCL 和 CUDA 之类的编程框架。这使得 GPU 也在 2010 年前后开始被机器学习社区使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet\n",
    "\n",
    "2012年，AlexNet 横空出世。这个模型的名字来源于论文第一作者的姓名 Alex Krizhevsky [1]。AlexNet 使用了8层卷积神经网络，并以很大的优势赢得了 ImageNet 2012 图像识别挑战赛。它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的前状。\n",
    "\n",
    "![](https://i.loli.net/2021/05/01/7hk4YDKS2XwVLvr.png)\n",
    "\n",
    "[1] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097-1105)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet 与 LeNet 的设计理念非常相似，但也有显著的区别。\n",
    "\n",
    "第一，与相对较小的 LeNet 相比，AlexNet 包含8层变换，其中有 5 层卷积和 2 层全连接隐藏层，以及 1 个全连接输出层。下面我们来详细描述这些层的设计。\n",
    "\n",
    "AlexNet第一层中的卷积窗口形状是 11×11。因为ImageNet中绝大多数图像的高和宽均比 MNIST 图像的高和宽大 10 倍以上，ImageNet 图像的物体占用更多的像素，所以需要更大的卷积窗口来捕获物体。第二层中的卷积窗口形状减小到 5×5，之后全采用 3×3。此外，第一、第二和第五个卷积层之后都使用了窗口形状为 3×3、步幅为 2 的最大池化层。而且，AlexNet 使用的卷积通道数也大于 LeNet 中的卷积通道数数十倍。\n",
    "\n",
    "紧接着最后一个卷积层的是两个输出个数为 4096 的全连接层。这两个巨大的全连接层带来将近 1 GB 的模型参数。由于早期显存的限制，最早的 AlexNet 使用双数据流的设计使一个 GPU 只需要处理一半模型。幸运的是，显存在过去几年得到了长足的发展，因此通常我们不再需要这样的特别设计了。\n",
    "\n",
    "第二，AlexNet 将 sigmoid 激活函数改成了更加简单的 ReLU 激活函数。一方面，ReLU 激活函数的计算更简单，例如它并没有 sigmoid 激活函数中的求幂运算。另一方面，ReLU 激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当 sigmoid 激活函数输出极接近 0 或 1 时，这些区域的梯度几乎为 0，从而造成反向传播无法继续更新部分模型参数；而 ReLU 激活函数在正区间的梯度恒为 1。因此，若模型参数初始化不当，sigmoid 函数可能在正区间得到几乎为 0 的梯度，从而令模型无法得到有效训练。\n",
    "\n",
    "第三，AlexNet 通过丢弃法来控制全连接层的模型复杂度。而 LeNet 并没有使用丢弃法。\n",
    "\n",
    "第四，AlexNet 引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。我们将在后面的一节（图像增广）详细介绍这种方法。\n",
    "\n",
    "下面我们实现稍微简化过的 AlexNet。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import dl4wm\n",
    "import os\n",
    "\n",
    "net = nn.Sequential(\n",
    "    # 这里，我们使用一个 11*11 的更大窗口来捕捉对象。\n",
    "    # 同时，步幅为 4，以减少输出的高度和宽度。\n",
    "    # 另外，输出通道的数目远大于 LeNet\n",
    "    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # 使用三个连续的卷积层和较小的卷积窗口。\n",
    "    # 除了最后的卷积层，输出通道的数量进一步增加。\n",
    "    # 在前两个卷积层之后，池化层不用于减少输入的高度和宽度\n",
    "    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),\n",
    "    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过度拟合\n",
    "    nn.Linear(6400, 4096), nn.ReLU(), nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5),\n",
    "    # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "    nn.Linear(4096, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们构造了一个高度和宽度都为 224 的单通道数据，来观察每一层输出的形状。它与 PPT 图中的 AlexNet 架构相匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d Output shape:\t torch.Size([1, 96, 54, 54])\n",
      "ReLU Output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d Output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Conv2d Output shape:\t torch.Size([1, 256, 26, 26])\n",
      "ReLU Output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d Output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Conv2d Output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU Output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d Output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU Output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d Output shape:\t torch.Size([1, 256, 12, 12])\n",
      "ReLU Output shape:\t torch.Size([1, 256, 12, 12])\n",
      "MaxPool2d Output shape:\t torch.Size([1, 256, 5, 5])\n",
      "Flatten Output shape:\t torch.Size([1, 6400])\n",
      "Linear Output shape:\t torch.Size([1, 4096])\n",
      "ReLU Output shape:\t torch.Size([1, 4096])\n",
      "Dropout Output shape:\t torch.Size([1, 4096])\n",
      "Linear Output shape:\t torch.Size([1, 4096])\n",
      "ReLU Output shape:\t torch.Size([1, 4096])\n",
      "Dropout Output shape:\t torch.Size([1, 4096])\n",
      "Linear Output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1, 1, 224, 224)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'Output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据集\n",
    "\n",
    "虽然论文中 AlexNet 使用 ImageNet 数据集，但因为 ImageNet 数据集训练时间较长，我们仍用前面的 Fashion-MNIST 数据集来演示 AlexNet。读取数据的时候我们额外做了一步将图像高和宽扩大到 AlexNet 使用的图像高和宽 224。这个可以通过 `torchvision.transforms.Resize` 实例来实现。也就是说，我们在 `ToTensor` 实例前使用 `Resize` 实例，然后使用 `Compose` 实例来将这两个变换串联以方便调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# 本函数已保存在 dl4wm 包中方便以后使用\n",
    "def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join('./', 'Datasets', 'FashionMNIST')):\n",
    "    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\n",
    "    root = os.path.expanduser(root)\n",
    "    transformer = []\n",
    "    if resize:\n",
    "        transformer += [transforms.Resize(resize)]\n",
    "    transformer += [transforms.ToTensor()]\n",
    "    transform = transforms.Compose(transformer)\n",
    "\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_iter, test_iter\n",
    "\n",
    "batch_size = 128//4\n",
    "# 如出现“out of memory”的报错信息，可减小 batch_size 或 resize\n",
    "train_iter, test_iter = dl4wm.load_data_fashion_mnist(batch_size, resize=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以开始训练 AlexNet 了。与上一讲中的 LeNet 相比，这里的主要变化是使用更小的学习速率训练，这是因为网络更深更广、图像分辨率更高，训练卷积神经网络就更昂贵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:3\n",
      "epoch 1, loss 0.0177, train acc 0.788, test acc 0.851, time 59.2 sec\n",
      "epoch 2, loss 0.0114, train acc 0.867, test acc 0.853, time 58.9 sec\n",
      "epoch 3, loss 0.0105, train acc 0.880, test acc 0.888, time 59.2 sec\n",
      "epoch 4, loss 0.0098, train acc 0.887, test acc 0.893, time 58.9 sec\n",
      "epoch 5, loss 0.0095, train acc 0.891, test acc 0.893, time 58.7 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "net.apply(init_weights)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=0.0005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dl4wm.train_gpu(net, train_iter, test_iter, criterion, optimizer, num_epochs, batch_size, dl4wm.try_gpu(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "\n",
    "- AlexNet 的结构与 LeNet 相似，但使用了更多的卷积层和更多的参数来拟合大规模的 ImageNet 数据集。\n",
    "- 今天，AlexNet 已经被更有效的结构所超越，但它是从浅层网络到深层网络的关键一步。\n",
    "- 尽管 AlexNet 的代码只比 LeNet 多出几行，但学术界花了很多年才接受深度学习这一概念，并应用其出色的实验结果。这也是由于缺乏有效的计算工具。\n",
    "- Dropout、ReLU 和预处理是提升计算机视觉任务性能的其他关键步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 使用块的网络（VGG）\n",
    "\n",
    ">(Restart your kernel here)\n",
    "\n",
    "虽然 AlexNet 证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。 在下面的几个章节中，我们将介绍一些常用于设计深层神经网络的启发式概念。\n",
    "\n",
    "与芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程类似，神经网络结构的设计也逐渐变得更加抽象。研究人员开始从单个神经元的角度思考问题，发展到整个层次，现在又转向模块，重复各层的模式。\n",
    "\n",
    "使用块的想法首先出现在牛津大学的视觉几何组（visualgeometry Group） (VGG)的 VGG网络 中。通过使用循环和子程序，可以很容易地在任何现代深度学习框架的代码中实现这些重复的结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG 块\n",
    "\n",
    "经典卷积神经网络的基本组成部分是下面的这个序列： \n",
    "\n",
    "1. 带填充以保持分辨率的卷积层； \n",
    "2. 非线性激活函数，如 ReLU； \n",
    "3. 池化层，如最大池化层。\n",
    "\n",
    "而一个 VGG 块与之类似，由一系列卷积层组成，后面再加上用于空间下采样的最大池化层。在最初的 VGG 论文 [Simonyan & Zisserman, 2014] 中，作者使用了带有  3×3  卷积核、填充为 1（保持高度和宽度）的卷积层，和带有  2×2  池化窗口、步幅为 2（每个块后的分辨率减半）的最大池化层。在下面的代码中，我们定义了一个名为 `vgg_block` 的函数来实现一个 VGG 块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import dl4wm\n",
    "\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该函数有三个参数，分别对应于卷积层的数量 `num_convs`、输入通道的数量 `in_channels` 和输出通道的数量 `out_channels`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG 网络\n",
    "\n",
    "与 AlexNet、LeNet 一样，VGG 网络可以分为两部分：第一部分主要由卷积层和池化层组成，第二部分由全连接层组成。如下图所示。\n",
    "\n",
    "![从AlexNet到VGG，它们本质上都是块设计。](https://i.loli.net/2021/05/01/ZAQ8KtaH7fCIlBW.png)\n",
    "\n",
    "（从AlexNet到VGG，它们本质上都是块设计。）\n",
    "\n",
    "VGG神经网络连续连接上图的几个 VGG 块（在 vgg_block 函数中定义）。其中有超参数变量 `conv_arch`。该变量指定了每个 VGG 块里卷积层个数和输出通道数。全连接模块则与 AlexNet 中的相同。\n",
    "\n",
    "原始 VGG 网络有 5 个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。 第一个模块有 64 个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到 512。由于该网络使用 8 个卷积层和 3 个全连接层，因此它通常被称为 VGG-11。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512)) # (卷积层数，输出的频道数)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的代码实现了 VGG-11。可以通过在 `conv_arch` 上执行 `for` 循环来简单实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    conv_blks = []\n",
    "    in_channels = 1\n",
    "    # 卷积层部分\n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(*conv_blks, nn.Flatten(),\n",
    "                         # 全连接层部分\n",
    "                         nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(),\n",
    "                         nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(),\n",
    "                         nn.Dropout(0.5), nn.Linear(4096, 10))\n",
    "\n",
    "net = vgg(conv_arch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将构建一个高度和宽度为 224 的单通道数据样本，以观察每个层输出的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 112, 112])\n",
      "Sequential output shape:\t torch.Size([1, 128, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 256, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 512, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "Flatten output shape:\t torch.Size([1, 25088])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(size=(1, 1, 224, 224))\n",
    "for blk in net:\n",
    "    X = blk(X)\n",
    "    print(blk.__class__.__name__, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如你所看到的，我们在每个块的高度和宽度减半，最终高度和宽度都为 7。最后再展平表示，送入全连接层处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于 VGG-11 比 AlexNet 计算量更大，因此我们构建了一个通道数较少的网络，足够用于训练 Fashion-MNIST 数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 16, 112, 112])\n",
      "Sequential output shape:\t torch.Size([1, 32, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 64, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 128, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 128, 7, 7])\n",
      "Flatten output shape:\t torch.Size([1, 6272])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "ratio = 4\n",
    "small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\n",
    "net = vgg(small_conv_arch)\n",
    "\n",
    "X = torch.randn(size=(1, 1, 224, 224))\n",
    "for blk in net:\n",
    "    X = blk(X)\n",
    "    print(blk.__class__.__name__, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了使用更小的学习率外，模型训练过程与上一节中的 AlexNet 类似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:3\n",
      "epoch 1, loss 0.0155, train acc 0.815, test acc 0.872, time 78.1 sec\n",
      "epoch 2, loss 0.0090, train acc 0.895, test acc 0.897, time 78.0 sec\n",
      "epoch 3, loss 0.0074, train acc 0.913, test acc 0.913, time 78.2 sec\n",
      "epoch 4, loss 0.0065, train acc 0.924, test acc 0.911, time 78.3 sec\n",
      "epoch 5, loss 0.0058, train acc 0.932, test acc 0.923, time 78.1 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs, batch_size = 0.0001, 5, 128//4\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# 如出现“out of memory”的报错信息，可减小 batch_size 或 resize\n",
    "train_iter, test_iter = dl4wm.load_data_fashion_mnist(batch_size, resize=224)\n",
    "net.apply(init_weights)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=0.0005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dl4wm.train_gpu(net, train_iter, test_iter, criterion, optimizer, num_epochs, batch_size, dl4wm.try_gpu(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "- VGG-11 使用可复用的卷积块构造网络。不同的 VGG 模型可通过每个块中卷积层数量和输出通道数量的差异来定义。\n",
    "\n",
    "- 块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。\n",
    "\n",
    "- 在 VGG 论文中，Simonyan 和 Ziserman 尝试了各种架构。特别是他们发现深层且窄的卷积（即 3×3 ）比较浅层且宽的卷积更有效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 网络中的网络 (NiN)\n",
    "\n",
    ">(Restart your kernel here)\n",
    "\n",
    "LeNet、AlexNet 和 VGG 都有一个共同的设计模式：通过一系列的卷积层与池化层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。 AlexNet 和 VGG 对 LeNet 的改进主要在于如何扩大和加深这两个模块。 或者，可以想象在这个过程的早期使用全连接层。 然而，如果使用稠密层了，可能会完全放弃表征的空间结构。 网络中的网络 (NiN) 提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机 [Lin et al., 2013]\n",
    "\n",
    ">Lin, M., Chen, Q., & Yan, S. (2013). Network in network. arXiv preprint arXiv:1312.4400."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NiN 块\n",
    "\n",
    "回想一下，卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本、通道、高度和宽度。 另外，全连接层的输入和输出通常是分别对应于样本和特征的二维张量。 NiN 的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。 如果我们将权重连接到每个空间位置，我们可以将其视为  1×1  卷积层（如上一讲中所述），或作为在每个像素位置上独立作用的全连接层。 从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）。\n",
    "\n",
    "下图说明了 VGG 和 NiN 及它们的块之间主要结构差异。 NiN 块以一个普通卷积层开始，后面是两个  1×1  的卷积层。这两个 1×1  卷积层充当带有 ReLU 激活函数的逐像素全连接层。 第一层的卷积窗口形状通常由用户设置。 随后的卷积窗口形状固定为  1×1 。\n",
    "\n",
    "![](https://i.loli.net/2021/05/01/j2IYfmgkhwaCSrN.png)\n",
    "\n",
    "（对比 VGG 和 NiN 及它们的块之间主要结构差异。）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import dl4wm\n",
    "\n",
    "def nin_block(in_channels, out_channels, kernel_size, strides, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),\n",
    "        nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "        nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "        nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NiN 模型\n",
    "\n",
    "最初的 NiN 网络是在 AlexNet 后不久提出的，显然从中得到了一些启示。 NiN使用窗口形状为  11×11 、 5×5  和  3×3 的卷积层，输出通道数量与 AlexNet 中的相同。 每个 NiN 块后有一个最大池化层，池化窗口形状为  3×3 ，步幅为 2。\n",
    "\n",
    "NiN 和 AlexNet 之间的一个显著区别是 NiN 完全取消了全连接层。 相反，NiN 使用一个 NiN块，其输出通道数等于标签类别的数量。最后放一个**全局平均池化层**（global average pooling layer），生成一个多元逻辑向量（logits）。NiN 设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nin_block(1, 96, kernel_size=11, strides=4, padding=0),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nin_block(96, 256, kernel_size=5, strides=1, padding=2),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nin_block(256, 384, kernel_size=3, strides=1, padding=1),\n",
    "    nn.MaxPool2d(3, stride=2), nn.Dropout(0.5),\n",
    "    # 标签类别数是10\n",
    "    nin_block(384, 10, kernel_size=3, strides=1, padding=1),\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    # 将四维的输出转成二维的输出，其形状为(批量大小, 10)\n",
    "    nn.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Sequential output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 384, 12, 12])\n",
      "MaxPool2d output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Dropout output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Sequential output shape:\t torch.Size([1, 10, 5, 5])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 10, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 224, 224))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练\n",
    "\n",
    "和以前一样，我们使用 Fashion-MNIST 来训练模型。训练 NiN 与训练 AlexNet、VGG时相似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:3\n",
      "epoch 1, loss 0.0324, train acc 0.640, test acc 0.792, time 45.6 sec\n",
      "epoch 2, loss 0.0175, train acc 0.802, test acc 0.805, time 46.1 sec\n",
      "epoch 3, loss 0.0149, train acc 0.828, test acc 0.833, time 46.8 sec\n",
      "epoch 4, loss 0.0133, train acc 0.848, test acc 0.862, time 46.6 sec\n",
      "epoch 5, loss 0.0124, train acc 0.857, test acc 0.867, time 46.2 sec\n",
      "epoch 6, loss 0.0115, train acc 0.869, test acc 0.874, time 46.3 sec\n",
      "epoch 7, loss 0.0110, train acc 0.873, test acc 0.876, time 46.7 sec\n",
      "epoch 8, loss 0.0105, train acc 0.880, test acc 0.881, time 46.4 sec\n",
      "epoch 9, loss 0.0101, train acc 0.885, test acc 0.870, time 46.3 sec\n",
      "epoch 10, loss 0.0097, train acc 0.887, test acc 0.885, time 46.3 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs, batch_size = 0.0001, 10, 128//4\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# 如出现“out of memory”的报错信息，可减小 batch_size 或 resize\n",
    "train_iter, test_iter = dl4wm.load_data_fashion_mnist(batch_size, resize=224)\n",
    "net.apply(init_weights)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=0.0005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dl4wm.train_gpu(net, train_iter, test_iter, criterion, optimizer, num_epochs, batch_size, dl4wm.try_gpu(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "\n",
    "- NiN 使用由一个卷积层和多个  1×1  卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。\n",
    "\n",
    "- NiN 去除了容易造成过拟合的全连接层，将它们替换为全局平均池化层（即在所有位置上进行求和）。该池化层通道数量为所需的输出数量（例如，Fashion-MNIST 的输出为 10）。\n",
    "\n",
    "- 移除全连接层可减少过拟合，同时显著减少 NiN 的参数。\n",
    "\n",
    "- NiN 的设计影响了许多后续卷积神经网络的设计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 含并行连结的网络（GoogLeNet）\n",
    "\n",
    ">(Restart your kernel here)\n",
    "\n",
    "在 2014 年的 ImageNet 图像识别挑战赛中，一个名叫 GoogLeNet [Szegedy et al., 2015] 的网络结构大放异彩。 GoogLeNet 吸收了NiN中串联网络的思想，并在此基础上做了改进。 这篇论文的一个重点是解决了什么样大小的卷积核最合适的问题。毕竟，以前流行的网络使用小到  1×1  ，大到  11×11  的卷积核。 本文的一个观点是，有时使用不同大小的卷积核组合是有利的。 \n",
    "\n",
    "在本节中，我们将介绍一个稍微简化的 GoogLeNet 版本：我们省略了一些为稳定训练而添加的特殊特性，但是现在有了更好的训练算法，这些特性不是必要的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Inception块\n",
    "\n",
    "在 GoogLeNet 中，基本的卷积块被称为 Inception 块（Inception block）。这很可能得名于电影《盗梦空间》（Inception），因为电影中的一句话“我们需要走得更深”（“We need to go deeper”）。\n",
    "\n",
    "![](https://i.loli.net/2021/05/01/bigpAs9C6VEhGBU.png)\n",
    "\n",
    "如上图所示，Inception 块由四条并行路径组成。 前三条路径使用窗口大小为  1×1 、 3×3  和  5×5  的卷积层，从不同空间大小中提取信息。 中间的两条路径在输入上执行  1×1  卷积，以减少通道数，从而降低模型的复杂性。 第四条路径使用  3×3  最大池化层，然后使用  1×1  卷积层来改变通道数。 这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成 Inception 块的输出。在 Inception 块中，通常调整的超参数是每层输出通道的数量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import dl4wm\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    # `c1`--`c4` 是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # 线路1，单1 x 1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1 x 1卷积层后接3 x 3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1 x 1卷积层后接5 x 5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3 x 3最大池化层后接1 x 1卷积层\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p1, p2, p3, p4), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么为什么 GoogLeNet 这个网络如此有效呢？ 首先我们考虑一下滤波器（filter）的组合，它们可以用各种滤波器尺寸探索图像，这意味着不同大小的滤波器可以有效地识别不同范围的图像细节。 同时，我们可以为不同的滤波器分配不同数量的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogLeNet 模型\n",
    "\n",
    "如下图所示，GoogLeNet 一共使用 9 个 Inception 块和全局平均池化层的堆叠来生成其估计值。Inception 块之间的最大池化层可降低维度。 第一个模块类似于 AlexNet 和 LeNet，Inception 块的栈从 VGG 继承，全局平均池化层避免了在最后使用全连接层。\n",
    "\n",
    "![](https://i.loli.net/2021/05/01/Sa5xynTEkMNmQso.png)\n",
    "\n",
    "现在，我们逐一实现 GoogLeNet 的每个模块。第一个模块使用 64 个通道、  7×7  卷积层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2,\n",
    "                                           padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个模块使用两个卷积层：第一个卷积层是 64个通道、  1×1  卷积层；第二个卷积层使用将通道数量增加三倍的  3×3  卷积层。 这对应于 Inception 块中的第二条路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1), nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三个模块串联两个完整的Inception块。 第一个 Inception 块的输出通道数为  64+128+32+32=256 ，四个路径之间的输出通道数量比为  64:128:32:32=2:4:1:1 。 第二个和第三个路径首先将输入通道的数量分别减少到  96/192=1/2  和  16/192=1/12 ，然后连接第二个卷积层。第二个 Inception 块的输出通道数增加到  128+192+96+64=480 ，四个路径之间的输出通道数量比为  128:192:96:64=4:6:3:2 。 第二条和第三条路径首先将输入通道的数量分别减少到  128/256=1/2  和  32/256=1/8 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception(256, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第四模块更加复杂， 它串联了5个Inception块，其输出通道数分别是  192+208+48+64=512  、  160+224+64+64=512  、  128+256+64+64=512  、  112+288+64+64=528  和  256+320+128+128=832  。 这些路径的通道数分配和第三模块中的类似，首先是含  3×3  卷积层的第二条路径输出最多通道，其次是仅含  1×1  卷积层的第一条路径，之后是含  5×5  卷积层的第三条路径和含  3×3  最大池化层的第四条路径。 其中第二、第三条路径都会先按比例减小通道数。 这些比例在各个 Inception 块中都略有不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第五模块包含输出通道数为  256+320+128+128=832  和  384+384+128+128=1024  的两个Inception块。 其中每条路径通道数的分配思路和第三、第四模块中的一致，只是在具体数值上有所不同。 需要注意的是，第五模块的后面紧跟输出层，该模块同 NiN 一样使用全局平均池化层，将每个通道的高和宽变成1。 最后我们将输出变成二维数组，再接上一个输出个数为标签类别数的全连接层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten())\n",
    "\n",
    "net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet 模型的计算复杂，而且不如 VGG 那样便于修改通道数。 为了在Fashion-MNIST上有一个合理的训练时间，我们将输入的高和宽从 224 降到 96，这简化了计算。下面演示各个模块输出的形状变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 24, 24])\n",
      "Sequential output shape:\t torch.Size([1, 192, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 480, 6, 6])\n",
      "Sequential output shape:\t torch.Size([1, 832, 3, 3])\n",
      "Sequential output shape:\t torch.Size([1, 1024])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 96, 96))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练\n",
    "\n",
    "和以前一样，我们使用 Fashion-MNIST 数据集来训练我们的模型。在训练之前，我们将图片转换为  96×96  分辨率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:3\n",
      "epoch 1, loss 0.0318, train acc 0.748, test acc 0.801, time 216.9 sec\n",
      "epoch 2, loss 0.0193, train acc 0.849, test acc 0.853, time 213.1 sec\n",
      "epoch 3, loss 0.0161, train acc 0.875, test acc 0.868, time 215.3 sec\n",
      "epoch 4, loss 0.0142, train acc 0.890, test acc 0.885, time 217.5 sec\n",
      "epoch 5, loss 0.0130, train acc 0.899, test acc 0.896, time 207.7 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs, batch_size = 0.0001, 5, 128//6\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# 如出现“out of memory”的报错信息，可减小 batch_size 或 resize\n",
    "train_iter, test_iter = dl4wm.load_data_fashion_mnist(batch_size, resize=96)\n",
    "net.apply(init_weights)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=0.0005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dl4wm.train_gpu(net, train_iter, test_iter, criterion, optimizer, num_epochs, batch_size, dl4wm.try_gpu(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "\n",
    "- Inception 块相当于一个有 4 条路径的子网络。它通过不同窗口形状的卷积层和最大池化层来并行抽取信息，并使用  1×1  卷积层减少每像素级别上的通道维数从而降低模型复杂度。\n",
    "\n",
    "- GoogLeNet 将多个设计精细的 Inception 块与其他层（卷积层、全连接层）串联起来。其中 Inception 块的通道数分配之比是在 ImageNet 数据集上通过大量的实验得来的。\n",
    "\n",
    "- GoogLeNet 和它的后继者们一度是 ImageNet 上最有效的模型之一：它以较低的计算复杂度提供了类似的测试精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 批量归一化\n",
    "\n",
    ">(Restart your kernel here)\n",
    "\n",
    "训练深层神经网络是十分困难的，特别是在较短的时间内使他们收敛更加棘手。 在本节中，我们将介绍批量归一化（batch normalization） [Ioffe & Szegedy, 2015] ，这是一种流行且有效的技术，可持续加速深层网络的收敛速度。 再结合在下一节中将介绍的残差块，批量归一化使得研究人员能够训练 100 层以上的网络。\n",
    "\n",
    ">Ioffe, S., & Szegedy, C. (2015). Batch normalization: accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从零开始实现\n",
    "\n",
    "下面我们自己实现批量归一化层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import dl4wm\n",
    "\n",
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 通过 `is_grad_enabled` 来判断当前模式是训练模式还是预测模式\n",
    "    if not torch.is_grad_enabled():\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean)**2).mean(dim=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。\n",
    "            # 这里我们需要保持X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            var = ((X - mean)**2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "        # 训练模式下，用当前的均值和方差做标准化\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # 缩放和移位\n",
    "    return Y, moving_mean.data, moving_var.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在可以创建一个正确的 BatchNorm 图层。 这个层将保持适当的参数：拉伸 `gamma` 和偏移 `beta`, 这两个参数将在训练过程中更新。 此外，我们的图层将保存均值和方差的移动平均值，以便在模型预测期间随后使用。\n",
    "\n",
    "撇开算法细节，注意我们实现图层的基础设计模式。 通常情况下，我们用一个单独的函数定义其数学原理，比如说 `batch_norm`。 然后，我们将此功能集成到一个自定义层中，其代码主要处理簿记问题，例如将数据移动到训练设备（如 GPU）、分配和初始化任何必需的变量、跟踪移动平均线（此处为均值和方差）等。 为了方便起见，我们并不担心在这里自动推断输入形状，因此我们需要指定整个特征的数量。 不用担心，深度学习框架中的批量归一化 API 将为我们解决上述问题，我们稍后将展示这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    # `num_features`：完全连接层的输出数量或卷积层的输出通道数。\n",
    "    # `num_dims`：2表示完全连接层，4表示卷积层\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 非模型参数的变量初始化为0和1\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 如果 `X` 不在内存上，将 `moving_mean` 和 `moving_var`\n",
    "        # 复制到 `X` 所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # 保存更新过的 `moving_mean` 和 `moving_var`\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean, self.moving_var,\n",
    "            eps=1e-5, momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用批量归一化层的 LeNet\n",
    "\n",
    "为了更好理解如何应用 BatchNorm，下面我们将其应用于 LeNet 模型。 回想一下，批量归一化是在卷积层或全连接层之后、相应的激活函数之前应用的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4),\n",
    "                    nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                    nn.Conv2d(6, 16,\n",
    "                              kernel_size=5), BatchNorm(16, num_dims=4),\n",
    "                    nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                    nn.Flatten(), nn.Linear(16 * 4 * 4, 120),\n",
    "                    BatchNorm(120, num_dims=2), nn.Sigmoid(),\n",
    "                    nn.Linear(120, 84), BatchNorm(84, num_dims=2),\n",
    "                    nn.Sigmoid(), nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和以前一样，我们将在 Fashion-MNIST 数据集训练我们的网络。 这个代码与我们第一次训练 LeNet 时几乎完全相同，主要区别在于学习率大得多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:3\n",
      "epoch 1, loss 0.0165, train acc 0.826, test acc 0.859, time 35.0 sec\n",
      "epoch 2, loss 0.0119, train acc 0.866, test acc 0.856, time 32.3 sec\n",
      "epoch 3, loss 0.0111, train acc 0.875, test acc 0.850, time 35.6 sec\n",
      "epoch 4, loss 0.0106, train acc 0.883, test acc 0.876, time 35.1 sec\n",
      "epoch 5, loss 0.0102, train acc 0.884, test acc 0.870, time 36.4 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs, batch_size = 0.001, 5, 128//4\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# 如出现“out of memory”的报错信息，可减小 batch_size 或 resize\n",
    "train_iter, test_iter = dl4wm.load_data_fashion_mnist(batch_size)\n",
    "net.apply(init_weights)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=0.0005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dl4wm.train_gpu(net, train_iter, test_iter, criterion, optimizer, num_epochs, batch_size, dl4wm.try_gpu(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 争议\n",
    "\n",
    "直观地说，批量归一化被认为可以使优化更加平滑。 然而，我们必须小心区分投机直觉和对我们观察到的现象的真实解释。回想一下，我们甚至不知道为什么简单的神经网络（多层感知机和传统的卷积神经网络）为什么如此有效。 即使在 dropout 和权重衰减的情况下，它们仍然非常灵活，因此无法通过传统的学习理论泛化保证来解释它们是否能够概括到看不见的数据。\n",
    "\n",
    "在提出批量归一化的论文中，作者除了介绍了其应用，还解释了其原理：通过减少**内部协变量偏移**（internal covariate shift）。 据推测，作者所说的“内部协变量转移”类似于上述的投机直觉，即变量值的分布在训练过程中会发生变化。 然而，这种解释有两个问题： \n",
    "\n",
    "1. 这种偏移与严格定义的**协变量偏移**（covariate shift）非常不同，所以这个名字用词不当。\n",
    "2. 这种解释只提供了一种不明确的直觉，但留下了一个有待后续挖掘的问题：为什么这项技术如此有效？。 \n",
    "\n",
    "本课程旨在传达实践者用来发展深层神经网络的直觉。然而，重要的是将这些指导性直觉与既定的科学事实区分开来。 最终，当你掌握了这些方法，并开始撰写自己的研究论文时，你会希望清楚地区分技术和直觉。\n",
    "\n",
    "随着批量归一化的普及，“内部协变量偏移”的解释反复出现在技术文献的辩论，特别是关于“如何展示机器学习研究”的更广泛的讨论中。 Ali Rahimi 在接受 2017 年 NeurIPS 大会的“接受时间考验奖”（Test of Time Award）时发表了一篇令人难忘的演讲。他将“内部协变量转移”作为焦点，将现代深度学习的实践比作炼金术。 他对该示例进行了详细回顾 [Lipton & Steinhardt, 2018] ，概述了机器学习中令人不安的趋势。 此外，一些作者对批量归一化的成功提出了另一种解释：在某些方面，批量归一化的表现出与原始论文 [Santurkar et al., 2018] 中声称的行为是相反的。\n",
    "\n",
    "然而，与技术机器学习文献中成千上万类似模糊的声明相比，内部协变量偏移没有什么更值得批评。 很可能，它作为这些辩论的焦点而产生共鸣，要归功于它对目标受众的广泛认可。 批量归一化已经证明是一种不可或缺的方法，适用于几乎所有图像分类器，在学术界获得了数万引用。\n",
    "\n",
    "> Lipton, Z. C., & Steinhardt, J. (2018). Troubling trends in machine learning scholarship. arXiv preprint arXiv:1807.03341.\n",
    ">\n",
    "> Santurkar, S., Tsipras, D., Ilyas, A., & Madry, A. (2018). How does batch normalization help optimization? Advances in Neural Information Processing Systems (pp. 2483–2493)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "\n",
    "- 在模型训练过程中，批量归一化利用小批量的均值和标准差，不断调整神经网络的中间输出，使整个神经网络各层的中间输出值更加稳定。\n",
    "\n",
    "- 批量归一化在全连接层和卷积层的使用略有不同。\n",
    "\n",
    "- 批量归一化层和 dropout 层一样，在训练模式和预测模式下计算不同。\n",
    "\n",
    "- 批量归一化有许多有益的副作用，主要是正则化。另一方面，”减少内部协变量偏移“的原始动机似乎不是一个有效的解释。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 残差网络 (ResNet)\n",
    "\n",
    ">(Restart your kernel here)\n",
    "\n",
    "让我们先思考一个问题：对神经网络模型添加新的层，充分训练后的模型是否只可能更有效地降低训练误差？理论上，原模型解的空间只是新模型解的空间的子空间。也就是说，如果我们能将新添加的层训练成恒等映射 $f(x)=x$，新模型和原模型将同样有效。由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。然而在实践中，添加过多的层后训练误差往往不降反升。即使利用批量归一化带来的数值稳定性使训练深层模型更加容易，该问题仍然存在。针对这一问题，何恺明等人提出了残差网络（ResNet）。它在 2015 年的 ImageNet 图像识别挑战赛夺魁，并深刻影响了后来的深度神经网络的设计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 残差块\n",
    "\n",
    "让我们聚焦于神经网络局部：如下图所示，假设我们的原始输入为 $x$，而希望学出的理想映射为 $f(x)$  （作为图中上方激活函数的输入）。图中左图虚线框中的部分需要直接拟合出该映射 $f(x)$ ，而右图虚线框中的部分则需要拟合出残差映射 $f(x)-x$。 残差映射在现实中往往更容易优化。 以本节开头提到的恒等映射作为我们希望学出的理想映射 $f(x)$，我们只需将图中右图虚线框内上方的加权运算（如仿射）的权重和偏置参数设成 0，那么 $f(x)$ 即为恒等映射。 实际中，当理想映射  $f(x)$ 极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。 图中右图是 ResNet 的基础结构—— **残差块**（residual block）。 在残差块中，输入可通过跨层数据线路更快地向前传播。\n",
    "\n",
    "![](https://i.loli.net/2021/05/04/FPb27XMfslN3t84.png)\n",
    "\n",
    "（一个正常块（左图）和一个残差块（右图））\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet 沿用了 VGG 完整的  3×3  卷积层设计。 残差块里首先有 2 个有相同输出通道数的  3×3  卷积层。 每个卷积层后接一个批量归一化层和 ReLU 激活函数。 然后我们通过跨层数据通路，跳过这 2 个卷积运算，将输入直接加在最后的 ReLU 激活函数前。 这样的设计要求 2 个卷积层的输出与输入形状一样，从而可以相加。 如果想改变通道数，就需要引入一个额外的  1×1  卷积层来将输入变换成需要的形状后再做相加运算。 残差块的实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import dl4wm\n",
    "\n",
    "class Residual(nn.Module):  # 本类已保存在 dl4wm 包中方便以后使用\n",
    "    def __init__(self, input_channels, num_channels, use_1x1conv=False,\n",
    "                 strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3,\n",
    "                               padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3,\n",
    "                               padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下图所示，此代码生成两种类型的网络： \n",
    "\n",
    "- 一种是在 `use_1x1conv=False` 、应用 ReLU 非线性函数之前，将输入添加到输出。 \n",
    "- 另一种是在 `use_1x1conv=True` 时，添加通过  1×1  卷积调整通道和分辨率。\n",
    "\n",
    "![](https://i.loli.net/2021/05/04/BpzA2LMyQXTcRk6.png)\n",
    "\n",
    "（包含以及不包含  1×1  卷积层的残差块）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们来查看输入和输出形状一致的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3, 3)\n",
    "X = torch.rand(4, 3, 6, 6)\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以在增加输出通道数的同时减半输出的高和宽。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 3, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3, 6, use_1x1conv=True, strides=2)\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet 模型\n",
    "\n",
    "ResNet 的前两层跟之前介绍的 GoogLeNet 中的一样： 在输出通道数为 64、步幅为 2 的  7×7  卷积层后，接步幅为 2 的  3×3  的最大池化层。 不同之处在于 ResNet 每个卷积层后增加了批量归一化层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet 在后面接了 4 个由Inception块组成的模块。 ResNet 则使用 4 个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。 第一个模块的通道数同输入通道数一致。 由于之前已经使用了步幅为 2 的最大池化层，所以无须减小高和宽。 之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。\n",
    "\n",
    "下面我们来实现这个模块。注意，我们对第一个模块做了特别处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                 first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(\n",
    "                Residual(input_channels, num_channels, use_1x1conv=True,\n",
    "                         strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着在 ResNet 加入所有残差块，这里每个模块使用 2 个残差块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "b5 = nn.Sequential(*resnet_block(256, 512, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，与 GoogLeNet 一样，在 ResNet 中加入全局平均池化层，以及全连接层输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(b1, b2, b3, b4, b5, nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                    nn.Flatten(), nn.Linear(512, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个模块有 4 个卷积层（不包括恒等映射的  1×1  卷积层）。 加上第一个  7×7  卷积层和最后一个全连接层，共有 18 层。 因此，这种模型通常被称为 ResNet-18。 通过配置不同的通道数和模块里的残差块数可以得到不同的 ResNet 模型，例如更深的含 152 层的 ResNet-152。 虽然 ResNet 的主体结构跟 GoogLeNet类似，但 ResNet 结构更简单，修改也更方便。这些因素都导致了 ResNet 迅速被广泛使用。 下图描述了完整的 ResNet-18。\n",
    "\n",
    "![](https://i.loli.net/2021/05/04/4EiLAXs9b78wuj1.png)\n",
    "\n",
    "（ResNet-18 架构）\n",
    "\n",
    "在训练 ResNet 之前，让我们观察一下 ResNet 中不同模块的输入形状是如何变化的。 在之前所有架构中，分辨率降低，通道数量增加，直到全局平均池化层聚集所有特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 128, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 256, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 512])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 224, 224))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练 ResNet\n",
    "\n",
    "同之前一样，我们在 Fashion-MNIST 数据集上训练 ResNet。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:3\n",
      "epoch 1, loss 0.0150, train acc 0.829, test acc 0.845, time 119.6 sec\n",
      "epoch 2, loss 0.0108, train acc 0.875, test acc 0.874, time 119.6 sec\n",
      "epoch 3, loss 0.0094, train acc 0.892, test acc 0.900, time 119.6 sec\n",
      "epoch 4, loss 0.0083, train acc 0.904, test acc 0.880, time 118.9 sec\n",
      "epoch 5, loss 0.0076, train acc 0.914, test acc 0.897, time 118.6 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs, batch_size = 0.001, 5, 256//8\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# 如出现“out of memory”的报错信息，可减小 batch_size 或 resize\n",
    "train_iter, test_iter = dl4wm.load_data_fashion_mnist(batch_size, resize=224)\n",
    "net.apply(init_weights)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=0.0005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dl4wm.train_gpu(net, train_iter, test_iter, criterion, optimizer, num_epochs, batch_size, dl4wm.try_gpu(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "\n",
    "- 学习嵌套函数（nested function）是训练神经网络的理想情况。在深层神经网络中，学习另一层作为恒等映射（identity function）较容易（尽管这是一个极端情况）。\n",
    "\n",
    "- 残差映射可以更容易地学习同一函数，例如将权重层中的参数近似为零。\n",
    "\n",
    "- 利用残差块（residual blocks）可以训练出一个有效的深层神经网络：输入可以通过层间的残余连接更快地向前传播。\n",
    "\n",
    "- 残差网络（ResNet）对随后的深层神经网络设计产生了深远影响，无论是卷积类网络还是全连接类网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 稠密连接网络（DenseNet）\n",
    "\n",
    ">(Restart your kernel here)\n",
    "\n",
    "ResNet极大地改变了如何参数化深层网络中函数的观点。 稠密连接网络 (DenseNet） [Huang et al., 2017] 在某种程度上是 ResNet 的逻辑扩展。让我们先从数学上了解一下。\n",
    "\n",
    ">Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700–4708).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从 ResNet 到 DenseNet\n",
    "\n",
    "回想一下任意函数的泰勒展开式（Taylor expansion），它把这个函数分解成越来越高阶的项。在  $x$  接近 $0$ 时，\n",
    "\n",
    "$$\n",
    "f(x) = f(0) + f'(0) x + \\frac{f''(0)}{2!}  x^2 + \\frac{f'''(0)}{3!}  x^3 + \\ldots.\n",
    "$$\n",
    "\n",
    "同样，ResNet 将函数展开为\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{x} + g(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "也就是说，ResNet 将 $f$ 分解为两部分：一个简单的线性项和一个更复杂的非线性项。 那么再向前拓展一步，如果我们想将 $f$ 拓展成超过两部分的信息呢？ 一种方案便是 DenseNet。\n",
    "\n",
    "![](https://i.loli.net/2021/05/04/uKrcBjMtVdil8Ce.png)\n",
    "\n",
    "（ResNet（左）与 DenseNet（右）在跨层连接上的主要区别：使用相加和使用连结。）\n",
    "\n",
    "如上图所示，ResNet 和 DenseNet 的关键区别在于，DenseNet 输出是连接（用图中的 [,]  表示）而不是如 ResNet 的简单相加。 因此，在应用越来越复杂的函数序列后，我们执行从 $x$ 到其展开式的映射：\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\to \\left[\n",
    "\\mathbf{x},\n",
    "f_1(\\mathbf{x}),\n",
    "f_2([\\mathbf{x}, f_1(\\mathbf{x})]), f_3([\\mathbf{x}, f_1(\\mathbf{x}), f_2([\\mathbf{x}, f_1(\\mathbf{x})])]), \\ldots\\right].\n",
    "$$\n",
    "\n",
    "最后，将这些展开式结合到多层感知机中，再次减少特征的数量。 实现起来非常简单：我们不需要添加术语，而是将它们连接起来。 DenseNet 这个名字由变量之间的“稠密连接”而得来，最后一层与之前的所有层紧密相连。 稠密连接如下图所示。\n",
    "\n",
    "![](https://i.loli.net/2021/05/04/PD8NYAomlXT1Vb7.png)\n",
    "\n",
    "（稠密连接）\n",
    "\n",
    "稠密网络主要由 2 部分构成： 稠密块（dense block）和 过渡层 （transition layer）。 前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 稠密块体\n",
    "\n",
    "DenseNet 使用了 ResNet 改良版的“批量归一化、激活和卷积”结构。 我们首先实现一下这个结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import dl4wm\n",
    "\n",
    "def conv_block(input_channels, num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(input_channels), nn.ReLU(),\n",
    "        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个稠密块由多个卷积块组成，每个卷积块使用相同数量的输出信道。 然而，在前向传播中，我们将每个卷积块的输入和输出在通道维上连结。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_convs, input_channels, num_channels):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        layer = []\n",
    "        for i in range(num_convs):\n",
    "            layer.append(\n",
    "                conv_block(num_channels * i + input_channels, num_channels))\n",
    "        self.net = nn.Sequential(*layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            # 连接通道维度上每个块的输入和输出\n",
    "            X = torch.cat((X, Y), dim=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下面的例子中，我们定义一个有 2 个输出通道数为 10 的 DenseBlock。 使用通道数为 3 的输入时，我们会得到通道数为  3+2×10=23  的输出。 卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为增长率（growth rate）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23, 8, 8])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = DenseBlock(2, 3, 10)\n",
    "X = torch.randn(4, 3, 8, 8)\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过渡层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。 而过渡层可以用来控制模型复杂度。 它通过  1×1  卷积层来减小通道数，并使用步幅为 2 的平均池化层减半高和宽，从而进一步降低模型复杂度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_block(input_channels, num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(input_channels), nn.ReLU(),\n",
    "        nn.Conv2d(input_channels, num_channels, kernel_size=1),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对上一个例子中稠密块的输出使用通道数为 10 的过渡层。 此时输出的通道数减为 10，高和宽均减半。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = transition_block(23, 10)\n",
    "blk(Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet模型\n",
    "\n",
    "我们来构造 DenseNet 模型。DenseNet 首先使用同 ResNet 一样的单卷积层和最大池化层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，类似于 ResNet 使用的 4 个残差块，DenseNet 使用的是 4 个稠密块。 与 ResNet 类似，我们可以设置每个稠密块使用多少个卷积层。 这里我们设成 4，从而与上一节的 ResNet-18 保持一致。 稠密块里的卷积层通道数（即增长率）设为 32，所以每个稠密块将增加 128 个通道。\n",
    "\n",
    "在每个模块之间，ResNet 通过步幅为 2 的残差块减小高和宽，DenseNet 则使用过渡层来减半高和宽，并减半通道数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `num_channels`为当前的通道数\n",
    "num_channels, growth_rate = 64, 32\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "blks = []\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    blks.append(DenseBlock(num_convs, num_channels, growth_rate))\n",
    "    # 上一个稠密块的输出通道数\n",
    "    num_channels += num_convs * growth_rate\n",
    "    # 在稠密块之间添加一个转换层，使通道数量减半\n",
    "    if i != len(num_convs_in_dense_blocks) - 1:\n",
    "        blks.append(transition_block(num_channels, num_channels // 2))\n",
    "        num_channels = num_channels // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与 ResNet 类似，最后接上全局池化层和全连接层来输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(b1, *blks, nn.BatchNorm2d(num_channels), nn.ReLU(),\n",
    "                    nn.AdaptiveMaxPool2d((1, 1)), nn.Flatten(),\n",
    "                    nn.Linear(num_channels, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "DenseBlock output shape:\t torch.Size([1, 192, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 96, 28, 28])\n",
      "DenseBlock output shape:\t torch.Size([1, 224, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 112, 14, 14])\n",
      "DenseBlock output shape:\t torch.Size([1, 240, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 120, 7, 7])\n",
      "DenseBlock output shape:\t torch.Size([1, 248, 7, 7])\n",
      "BatchNorm2d output shape:\t torch.Size([1, 248, 7, 7])\n",
      "ReLU output shape:\t torch.Size([1, 248, 7, 7])\n",
      "AdaptiveMaxPool2d output shape:\t torch.Size([1, 248, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 248])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 224, 224))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "\n",
    "由于这里使用了比较深的网络，本节里我们将输入高和宽从 224 降到 96 来简化计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:3\n",
      "epoch 1, loss 0.0097, train acc 0.851, test acc 0.889, time 76.9 sec\n",
      "epoch 2, loss 0.0068, train acc 0.895, test acc 0.899, time 76.6 sec\n",
      "epoch 3, loss 0.0061, train acc 0.906, test acc 0.897, time 78.1 sec\n",
      "epoch 4, loss 0.0058, train acc 0.911, test acc 0.910, time 77.6 sec\n",
      "epoch 5, loss 0.0054, train acc 0.917, test acc 0.897, time 76.5 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs, batch_size = 0.001, 5, 256//6\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# 如出现“out of memory”的报错信息，可减小 batch_size 或 resize\n",
    "train_iter, test_iter = dl4wm.load_data_fashion_mnist(batch_size, resize=96)\n",
    "net.apply(init_weights)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=0.0005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dl4wm.train_gpu(net, train_iter, test_iter, criterion, optimizer, num_epochs, batch_size, dl4wm.try_gpu(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "\n",
    "- 在跨层连接上，不同于 ResNet 中将输入与输出相加，稠密连接网络（DenseNet）在通道维上连结输入与输出。\n",
    "\n",
    "- DenseNet 的主要构建模块是稠密块和过渡层。\n",
    "\n",
    "- 在构建 DenseNet 时，我们需要通过添加过渡层来控制网络的维数，从而再次减少信道的数量。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
